"""
Performance Metrics Collection for Full-Duplex Voice Assistant
Prometheus-compatible metrics for monitoring and evaluation
"""

from prometheus_client import Counter, Histogram, Gauge, Summary, Info
from prometheus_client import CollectorRegistry, generate_latest, CONTENT_TYPE_LATEST
from functools import wraps
import time
import psutil
import os
from typing import Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)

# Create a custom registry for better control
registry = CollectorRegistry()

# ==================== Latency Metrics ====================

# Token minting latency (ephemeral token generation)
token_mint_latency = Histogram(
    'token_mint_latency_seconds',
    'Latency for minting ephemeral OpenAI tokens',
    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0],
    registry=registry
)

# OpenAI API call latency by endpoint type
openai_api_latency = Histogram(
    'openai_api_latency_seconds',
    'OpenAI API call latency by endpoint',
    ['endpoint', 'model'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0],
    registry=registry
)

# HTTP request latency
http_request_latency = Histogram(
    'http_request_latency_seconds',
    'HTTP request latency by endpoint and method',
    ['method', 'endpoint', 'status'],
    buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0],
    registry=registry
)

# End-to-end latency (from client metrics)
e2e_latency_client = Histogram(
    'e2e_latency_client_milliseconds',
    'End-to-end latency reported by client',
    buckets=[100, 250, 500, 750, 1000, 1500, 2000, 3000, 5000],
    registry=registry
)

# Barge-in latency
barge_in_latency = Histogram(
    'barge_in_latency_milliseconds',
    'Barge-in/interruption response time',
    buckets=[50, 100, 200, 300, 500, 750, 1000],
    registry=registry
)

# ==================== Throughput Metrics ====================

# API request counters
api_requests_total = Counter(
    'api_requests_total',
    'Total API requests by method, endpoint, and status',
    ['method', 'endpoint', 'status'],
    registry=registry
)

# WebSocket connections
websocket_connections_active = Gauge(
    'websocket_connections_active',
    'Number of active WebSocket connections',
    registry=registry
)

websocket_connections_total = Counter(
    'websocket_connections_total',
    'Total WebSocket connections established',
    registry=registry
)

websocket_disconnections_total = Counter(
    'websocket_disconnections_total',
    'Total WebSocket disconnections',
    ['reason'],
    registry=registry
)

# Message throughput
websocket_messages_sent = Counter(
    'websocket_messages_sent_total',
    'Total WebSocket messages sent',
    ['message_type'],
    registry=registry
)

websocket_messages_received = Counter(
    'websocket_messages_received_total',
    'Total WebSocket messages received',
    ['message_type'],
    registry=registry
)

# Audio data throughput
audio_bytes_sent = Counter(
    'audio_bytes_sent_total',
    'Total audio bytes sent to clients',
    registry=registry
)

audio_bytes_received = Counter(
    'audio_bytes_received_total',
    'Total audio bytes received from clients',
    registry=registry
)

# ==================== Quality Metrics ====================

# Conversation metrics
conversation_turns_total = Counter(
    'conversation_turns_total',
    'Total conversation turns',
    ['speaker'],  # user or assistant
    registry=registry
)

conversation_interruptions_total = Counter(
    'conversation_interruptions_total',
    'Total user interruptions (barge-ins)',
    registry=registry
)

# ASR metrics (when available from client)
asr_transcription_delay = Histogram(
    'asr_transcription_delay_milliseconds',
    'Time from speech end to final transcript',
    buckets=[100, 250, 500, 1000, 2000, 5000],
    registry=registry
)

# LLM generation metrics
llm_tokens_generated = Counter(
    'llm_tokens_generated_total',
    'Total tokens generated by LLM',
    ['model'],
    registry=registry
)

llm_generation_errors = Counter(
    'llm_generation_errors_total',
    'LLM generation errors',
    ['error_type'],
    registry=registry
)

# ==================== Streaming Stability ====================

# Packet loss and jitter (from client reports)
streaming_packet_loss_rate = Gauge(
    'streaming_packet_loss_rate',
    'Current packet loss rate (0-1)',
    registry=registry
)

streaming_jitter_milliseconds = Gauge(
    'streaming_jitter_milliseconds',
    'Current jitter measurement',
    registry=registry
)

streaming_underrun_events = Counter(
    'streaming_underrun_events_total',
    'Audio buffer underrun events',
    registry=registry
)

streaming_reconnections = Counter(
    'streaming_reconnections_total',
    'WebSocket reconnection events',
    registry=registry
)

# Network Quality Index
network_quality_index = Gauge(
    'network_quality_index',
    'Composite network quality score (0-1)',
    registry=registry
)

# ==================== Error Metrics ====================

# Rate limiting
rate_limit_exceeded = Counter(
    'rate_limit_exceeded_total',
    'Rate limit violations',
    ['endpoint', 'client_ip'],
    registry=registry
)

# Authentication errors
auth_errors_total = Counter(
    'auth_errors_total',
    'Authentication/authorization errors',
    ['error_type'],
    registry=registry
)

# HTTP errors
http_errors_total = Counter(
    'http_errors_total',
    'HTTP errors by status code',
    ['status_code', 'endpoint'],
    registry=registry
)

# Application errors
application_errors_total = Counter(
    'application_errors_total',
    'Application-level errors',
    ['error_type', 'component'],
    registry=registry
)

# ==================== Resource Metrics ====================

# System resource gauges
system_cpu_percent = Gauge(
    'system_cpu_percent',
    'System CPU utilization percentage',
    registry=registry
)

system_memory_bytes = Gauge(
    'system_memory_bytes',
    'System memory usage',
    ['type'],  # used, available, total
    registry=registry
)

process_memory_bytes = Gauge(
    'process_memory_bytes',
    'Process memory usage',
    ['type'],  # rss, vms
    registry=registry
)

process_cpu_percent = Gauge(
    'process_cpu_percent',
    'Process CPU utilization percentage',
    registry=registry
)

# ==================== Service Health ====================

service_info = Info(
    'service',
    'Service information',
    registry=registry
)

service_up = Gauge(
    'service_up',
    'Service health status (1=up, 0=down)',
    registry=registry
)

service_uptime_seconds = Gauge(
    'service_uptime_seconds',
    'Service uptime in seconds',
    registry=registry
)

# Track service start time
_service_start_time = time.time()
service_up.set(1)


# ==================== Helper Functions ====================

def track_time(metric: Histogram, labels: Optional[Dict[str, str]] = None):
    """
    Decorator to track execution time of functions

    Usage:
        @track_time(openai_api_latency, {'endpoint': 'chat', 'model': 'gpt-4'})
        async def call_openai():
            ...
    """
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            start = time.perf_counter()
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                duration = time.perf_counter() - start
                if labels:
                    metric.labels(**labels).observe(duration)
                else:
                    metric.observe(duration)

        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            start = time.perf_counter()
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                duration = time.perf_counter() - start
                if labels:
                    metric.labels(**labels).observe(duration)
                else:
                    metric.observe(duration)

        # Return appropriate wrapper based on function type
        import asyncio
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper

    return decorator


def update_resource_metrics():
    """Update system and process resource metrics"""
    try:
        # System metrics
        system_cpu_percent.set(psutil.cpu_percent(interval=None))

        mem = psutil.virtual_memory()
        system_memory_bytes.labels(type='used').set(mem.used)
        system_memory_bytes.labels(type='available').set(mem.available)
        system_memory_bytes.labels(type='total').set(mem.total)

        # Process metrics
        process = psutil.Process(os.getpid())
        mem_info = process.memory_info()
        process_memory_bytes.labels(type='rss').set(mem_info.rss)
        process_memory_bytes.labels(type='vms').set(mem_info.vms)
        process_cpu_percent.set(process.cpu_percent(interval=None))

        # Uptime
        uptime = time.time() - _service_start_time
        service_uptime_seconds.set(uptime)

    except Exception as e:
        logger.error(f"Failed to update resource metrics: {e}")


def record_client_metrics(metrics_data: Dict[str, Any]):
    """
    Record metrics received from client

    Args:
        metrics_data: Dictionary containing client-reported metrics
    """
    try:
        # End-to-end latency
        if 'latency' in metrics_data and 'endToEnd' in metrics_data['latency']:
            e2e = metrics_data['latency']['endToEnd']
            if 'p50' in e2e and e2e['p50'] is not None:
                e2e_latency_client.observe(e2e['p50'])

        # Barge-in latency
        if 'latency' in metrics_data and 'bargeIn' in metrics_data['latency']:
            barge = metrics_data['latency']['bargeIn']
            if 'mean' in barge and barge['mean'] > 0:
                barge_in_latency.observe(barge['mean'])

        # Streaming stability
        if 'streaming' in metrics_data:
            stream = metrics_data['streaming']

            if 'packetLossRate' in stream:
                streaming_packet_loss_rate.set(float(stream['packetLossRate']) / 100)

            if 'jitterMs' in stream and stream['jitterMs'] > 0:
                streaming_jitter_milliseconds.set(float(stream['jitterMs']))

            if 'underrunEvents' in stream:
                # This is cumulative, so we'd need to track delta
                pass

            if 'nqi' in stream:
                network_quality_index.set(float(stream['nqi']))

        # Quality metrics
        if 'quality' in metrics_data:
            qual = metrics_data['quality']

            if 'interruptions' in qual:
                conversation_interruptions_total.inc(qual['interruptions'])

        logger.debug(f"Recorded client metrics: {metrics_data.get('timestamp')}")

    except Exception as e:
        logger.error(f"Failed to record client metrics: {e}")


def set_service_info(version: str, environment: str):
    """Set service metadata"""
    service_info.info({
        'version': version,
        'environment': environment,
        'python_version': os.sys.version.split()[0]
    })


class MetricsMiddleware:
    """
    ASGI middleware to collect HTTP metrics
    """
    def __init__(self, app):
        self.app = app

    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return

        start_time = time.perf_counter()
        status_code = 500  # Default to error

        async def send_wrapper(message):
            nonlocal status_code
            if message["type"] == "http.response.start":
                status_code = message["status"]
            await send(message)

        try:
            await self.app(scope, receive, send_wrapper)
        finally:
            duration = time.perf_counter() - start_time

            # Extract route info
            method = scope.get("method", "UNKNOWN")
            path = scope.get("path", "unknown")

            # Record metrics
            api_requests_total.labels(
                method=method,
                endpoint=path,
                status=status_code
            ).inc()

            http_request_latency.labels(
                method=method,
                endpoint=path,
                status=status_code
            ).observe(duration)

            # Track errors
            if status_code >= 400:
                http_errors_total.labels(
                    status_code=status_code,
                    endpoint=path
                ).inc()


def get_metrics_response():
    """
    Generate Prometheus metrics response

    Returns:
        tuple: (content, media_type)
    """
    # Update resource metrics before generating response
    update_resource_metrics()

    metrics = generate_latest(registry)
    return metrics, CONTENT_TYPE_LATEST


# ==================== WebSocket Metrics Tracker ====================

class WebSocketMetricsTracker:
    """
    Context manager for tracking WebSocket connection metrics
    """
    def __init__(self, connection_id: str):
        self.connection_id = connection_id
        self.start_time = time.time()

    def __enter__(self):
        websocket_connections_total.inc()
        websocket_connections_active.inc()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        websocket_connections_active.dec()

        if exc_type is not None:
            reason = exc_type.__name__
        else:
            reason = "normal"

        websocket_disconnections_total.labels(reason=reason).inc()

        duration = time.time() - self.start_time
        logger.info(f"WebSocket {self.connection_id} closed after {duration:.2f}s")

    async def __aenter__(self):
        return self.__enter__()

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        return self.__exit__(exc_type, exc_val, exc_tb)


# ==================== Initialization ====================

def initialize_metrics(version: str = "1.0.0", environment: str = "production"):
    """
    Initialize metrics system

    Args:
        version: Service version
        environment: Deployment environment
    """
    set_service_info(version, environment)
    update_resource_metrics()
    logger.info("Metrics system initialized")


# Export commonly used metrics
__all__ = [
    'registry',
    'token_mint_latency',
    'openai_api_latency',
    'http_request_latency',
    'e2e_latency_client',
    'barge_in_latency',
    'api_requests_total',
    'websocket_connections_active',
    'rate_limit_exceeded',
    'track_time',
    'update_resource_metrics',
    'record_client_metrics',
    'get_metrics_response',
    'MetricsMiddleware',
    'WebSocketMetricsTracker',
    'initialize_metrics',
]
